\section{Image Formats and Convolutions}

A discrete two-dimensional convolution is a linear operation that produces an output image by sliding a small matrix, the kernel $K$ of size $(2a+1)\times(2b+1)$, over every pixel of an input image $I$ and computing a weighted sum of the neighborhood, as defined in Eq.~(\ref{eq:conv2d}):

\begin{equation}
\label{eq:conv2d}
(I * K)(y,x)=\sum_{j=-a}^{a}\sum_{i=-b}^{b} I(y+j,\,x+i)\;K(j,i).
\end{equation}

This operation is the computational basis for a range of image-processing tasks such as smoothing, sharpening, edge detection, and feature extraction, all depending on the choice of kernel \cite{gonzalez_woods_dip}.

\subsection{OpenCV and Matplotlib Functions}

The provided code in \texttt{Convolutions.py} relies on several OpenCV and Matplotlib functions whose behavior needs to be understood to interpret the results correctly.

The image is loaded with \texttt{cv2.imread(path, 0)}, where the flag \texttt{0} forces grayscale loading as a single-channel 8-bit array, and the result is cast to \texttt{float64} via \texttt{np.float64()} so that subsequent convolution operations can produce values outside $[0,255]$ without overflow. For copying the image, the code calls \texttt{cv2.copyMakeBorder\allowbreak(img,\allowbreak0,0,0,0,\allowbreak cv2.BORDER\_REPLICATE)} with zero padding on every side, which in practice just creates a deep copy; the \texttt{BORDER\_REPLICATE} flag would replicate edge pixels if positive padding were specified. The optimized convolution is done via \texttt{cv2.filter2D(img, -1, kernel)}, where \texttt{-1} means the output keeps the same depth as the input; internally, this delegates to OpenCV's C++ backend, which can use SIMD and multi-threading \cite{opencv_filter2d}.

\subsection{Direct Computation vs OpenCV filter2D}

The code compares two ways of computing the convolution of the grayscale image \texttt{FlowerGarden2.png} ($240\times360$ pixels) with the $3\times3$ sharpening kernel. The first is a direct method thaimitationt uses a nested Python loop over all interior pixels $(y,x)$ with $1 \le y \le h{-}2$, $1 \le x \le w{-}2$, computing explicitly:

\begin{equation}
\label{eq:direct_val}
v = 5\,I(y,x) - I(y{-}1,x) - I(y,x{-}1) - I(y{+}1,x) - I(y,x{+}1),
\end{equation}

followed by a clamp $I_{\text{out}}(y,x) = \min(\max(v,\,0),\,255)$. The second is a single call to \texttt{cv2.filter2D}, which does the same operation but through OpenCV's compiled C++ backend.

Given the nature of these two implementations, the hypothesis is straightforward: the direct Python loop should be dramatically slower than \texttt{filter2D}, because every pixel iteration goes through the Python interpreter with dynamic type checking, whereas the C++ path processes contiguous memory with compiled instructions and can leverage SIMD vectorization and multi-threading. We would also expect that, for a small image like this one, enabling or disabling multi-threading in OpenCV should not make a large difference, since the overhead of thread synchronization could offset the parallelism gains when the workload per thread is small.

To test this, each method was benchmarked over multiple independent runs (5 for the slow direct method, 20 for the fast \texttt{filter2D} variants), and the median execution time was retained to mitigate warm-up and scheduling noise. A single-thread variant of \texttt{filter2D} was measured separately by calling \texttt{cv2.setNumThreads(1)} before execution.

\begin{table}[!t]
\centering
\caption{Median execution time for the convolution of a $240\times360$ grayscale image with a $3\times3$ kernel.}
\label{tab:conv-timing}
\begin{tabular}{lcr}
\hline
Method & Median time & Runs \\
\hline
Direct (Python loop)       & 0.6987\,s   &  5 \\
\texttt{filter2D} (multi-thread) & 0.329\,ms  & 20 \\
\texttt{filter2D} (1-thread)     & 0.348\,ms  & 20 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/timing_comparison.png}
  \caption{Median execution time comparison (log scale) for the three convolution methods on a $240\times360$ image.}
  \label{fig:timing-comparison}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/timing_boxplot.png}
  \caption{Distribution of execution times across all benchmark runs for each method.}
  \label{fig:timing-boxplot}
\end{figure}

As reported in Table~\ref{tab:conv-timing} and illustrated in Fig.~\ref{fig:timing-comparison}, the direct Python loop takes about $0.70$\,s while \texttt{filter2D} completes in roughly $0.33$\,ms, giving a speedup of approximately $2\,100\times$. This is consistent with the hypothesis: the per-pixel overhead of the Python interpreter is enormous compared to the compiled, vectorized C++ implementation. The difference between the multi-thread and single-thread variants of \texttt{filter2D} is almost negligible ($0.329$\,ms vs $0.348$\,ms), which also confirms our expectation that thread synchronization cost offsets any parallelism benefit at this image size. Moreover, Fig.~\ref{fig:timing-boxplot} shows that the \texttt{filter2D} measurements have very low variance, reflecting the stability of a compiled implementation, while the direct method exhibits more dispersion---likely caused by Python's garbage collector and OS scheduling interrupting the long-running loop.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/time_vs_image_size.png}
  \caption{Execution time as a function of image resolution (25\,\%, 50\,\%, 75\,\%, and 100\,\% of the original $240\times360$ image).}
  \label{fig:time-vs-size}
\end{figure}

To see how these results scale, the benchmark was repeated on resized versions of the image at 25\,\%, 50\,\%, 75\,\%, and 100\,\% of the original resolution. The hypothesis here is that the direct method should grow roughly quadratically with the number of pixels (since it visits every pixel once in a doubly-nested loop), whereas \texttt{filter2D} should remain nearly flat at these small sizes because the actual convolution time is dwarfed by the fixed overhead of the function call. As shown in Fig.~\ref{fig:time-vs-size}, this is exactly what happens: the direct method curve grows steeply as the resolution increases, consistent with $\mathcal{O}(H \cdot W \cdot k^2)$ complexity, while both \texttt{filter2D} variants stay close to the bottom of the plot across all tested resolutions.

\subsection{Visual and Numerical Comparison}

Since both methods are computing the same mathematical convolution, the hypothesis is that their outputs should be identical in the interior of the image, and that any differences should appear only at the borders due to the different ways each method handles boundary pixels. The direct loop simply skips the border (it only iterates from $y=1$ to $h{-}2$) and leaves the original values there via the initial copy, whereas \texttt{filter2D} applies its own default border extrapolation (\texttt{BORDER\_REFLECT\_101}) before computing the convolution, so the border values will differ.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/visual_comparison.png}
  \caption{Visual comparison: original image (top left), direct method result (top right), \texttt{filter2D} result (bottom left), and absolute difference amplified $\times10$ (bottom right).}
  \label{fig:visual-comparison}
\end{figure}

The results in Fig.~\ref{fig:visual-comparison} confirm this. A pixel-wise comparison shows that $98.9\,\%$ of the pixels are numerically identical between the two outputs, with a mean absolute difference of only $0.575$ gray levels. The maximum difference reaches $153$ gray levels, but as visible in the amplified difference heatmap (bottom right of Fig.~\ref{fig:visual-comparison}), these large differences are concentrated exclusively at the image borders. In the interior, both methods produce the same result, which makes sense because the convolution formula is the same---the only divergence comes from boundary handling, not from the computation itself. This confirms that the performance gap discussed in the previous subsection is purely an implementation-level phenomenon and does not affect the mathematical result.

\subsection{Contrast Enhancement by Unsharp Masking}

The goal of this subsection is to explain \emph{why} the $3\times 3$ convolution kernel used in the TP code enhances contrast. The kernel is:
\begin{equation}
\label{eq:kernel}
K=
\begin{pmatrix}
 0 & -1 &  0 \\
-1 &  5 & -1 \\
 0 & -1 &  0
\end{pmatrix}.
\end{equation}
At first glance the coefficients may seem arbitrary, but they have a precise algebraic structure: $K$ can be decomposed as the identity minus a discrete Laplacian. This decomposition reveals that $K$ implements \emph{unsharp masking}, a classical technique that preserves low-frequency content (uniform regions) while amplifying high-frequency content (edges).

\paragraph{Step 1 --- The discrete Laplacian.}
The continuous Laplacian of a function $f(x,y)$ is
\(
\Delta f=\frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}.
\)
On a discrete pixel grid, each second derivative is approximated by a centered finite difference (e.g.\ $\frac{\partial^2 f}{\partial x^2}\approx f[i,j{+}1]-2f[i,j]+f[i,j{-}1]$), and the sum of both gives the 4-connected Laplacian stencil:
\begin{equation}
\label{eq:L4_def}
L_4 =
\begin{pmatrix}
 0 &  1 &  0 \\
 1 & -4 &  1 \\
 0 &  1 &  0
\end{pmatrix},
\qquad
(f \star L_4)[i,j] \approx \Delta f(i,j).
\end{equation}
Two properties of $L_4$ are important: its coefficients sum to zero (so it has no DC response: a constant image gives zero output), and its output is \emph{signed} (positive where the pixel is darker than its neighbors, negative where it is brighter).

\paragraph{Step 2 --- Decomposing $K$ as identity minus Laplacian.}
Let $\delta$ denote the identity kernel
\(
\delta=
\begin{psmallmatrix}
0&0&0\\
0&1&0\\
0&0&0
\end{psmallmatrix}.
\)
Subtracting $L_4$ from $\delta$ coefficient by coefficient:
\begin{equation}
\label{eq:kernel_decomp}
\delta - L_4
=
\begin{psmallmatrix}0&0&0\\0&1&0\\0&0&0\end{psmallmatrix}
-
\begin{psmallmatrix}0&1&0\\1&{-4}&1\\0&1&0\end{psmallmatrix}
=
\begin{psmallmatrix}0&{-1}&0\\{-1}&5&{-1}\\0&{-1}&0\end{psmallmatrix}
= K.
\end{equation}
This is verified numerically in Fig.~\ref{fig:kernel-decomp}. Since convolution is linear, the output of filtering with $K$ can be split into two terms:
\begin{equation}
\label{eq:unsharp}
f_{\text{out}}
= f \star K
= f \star (\delta - L_4)
= \underbrace{f \star \delta}_{=\,f}
  \;-\;
  \underbrace{f \star L_4}_{\approx\,\Delta f}
= f - (f \star L_4).
\end{equation}
This is exactly the \emph{Laplacian-based unsharp masking} formula $f_{\text{out}} = f - \gamma\,\Delta f$ with gain $\gamma=1$.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/kernel_decomposition.png}
  \caption{Kernel decomposition: $K=\delta-L_4$ (4-neighborhood Laplacian).}
  \label{fig:kernel-decomp}
\end{figure}

\paragraph{Step 3 --- Why does this enhance contrast? (pixel-level view).}
To understand the effect intuitively, we can rewrite Eq.~\eqref{eq:unsharp} in terms of the local neighborhood. Writing out the Laplacian convolution at pixel $[i,j]$:
\begin{equation}
\label{eq:laplacian_pixel}
(f \star L_4)[i,j]
=
f[i{+}1,j]+f[i{-}1,j]+f[i,j{+}1]+f[i,j{-}1]-4f[i,j].
\end{equation}
If we define the mean of the four direct neighbors as
\begin{equation}
\label{eq:neighbor_mean}
\overline{f}_{N_4}[i,j]
=
\tfrac{1}{4}\!\left(f[i{+}1,j]+f[i{-}1,j]+f[i,j{+}1]+f[i,j{-}1]\right),
\end{equation}
then the Laplacian simplifies to $(f \star L_4)[i,j]=4\bigl(\overline{f}_{N_4}[i,j]-f[i,j]\bigr)$. Substituting into Eq.~\eqref{eq:unsharp}:
\begin{equation}
\label{eq:local_boost}
f_{\text{out}}[i,j]
=
f[i,j] \;+\; 4\!\left(f[i,j]-\overline{f}_{N_4}[i,j]\right).
\end{equation}
Equation~\eqref{eq:local_boost} is the key: the output equals the original value plus a correction proportional to how much the pixel \emph{differs from its local mean}. Three cases arise:
\begin{itemize}
\item \textbf{Smooth region} ($f[i,j]\approx\overline{f}_{N_4}$): the correction is nearly zero, so $f_{\text{out}}\approx f$. Uniform areas are preserved.
\item \textbf{Locally bright pixel} ($f[i,j]>\overline{f}_{N_4}$): the correction is positive, pushing the pixel \emph{brighter}.
\item \textbf{Locally dark pixel} ($f[i,j]<\overline{f}_{N_4}$): the correction is negative, pushing the pixel \emph{darker}.
\end{itemize}
In short, pixels are pushed \emph{away} from their local mean. At an edge, the bright side becomes brighter and the dark side becomes darker, which is exactly what ``enhancing contrast'' means.

\paragraph{Experimental results.}
Fig.~\ref{fig:unsharp-components} shows the decomposition applied to the test image: the Laplacian response (panel~b) is signed and concentrated at edges, while the sharpened result (panel~d) shows visibly crisper contours.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/unsharp_components.png}
  \caption{Unsharp masking components: (a)~original $f$, (b)~Laplacian $f\star L_4$ (signed, divergent colormap), (c)~$|f\star L_4|$ rescaled to $[0,255]$, (d)~sharpened result $f-(f\star L_4)$ after clipping.}
  \label{fig:unsharp-components}
\end{figure}

\paragraph{Dynamic-range considerations.}
Since the filter pushes pixels away from their local mean (Eq.~\eqref{eq:local_boost}), the output distribution necessarily becomes wider than the input. Near strong edges, the Laplacian response can be large, causing $f_{\text{out}}$ to exceed the $[0,255]$ range. Table~\ref{tab:hist-stats} quantifies this: before clipping, values range from $-609$ to $877$, and the standard deviation doubles from $65.90$ to $136.08$. After clipping to $[0,255]$, the extreme values accumulate at the boundaries (visible as a saturation peak at bin~$255$ in Fig.~\ref{fig:histograms}b), and the mean shifts slightly upward ($127.14 \to 129.68$) because clipping is asymmetric. The DC-gain-equals-one property ($\widehat{K}(0,0)=1$) guarantees that the mean is exactly preserved in the float domain ($127.14 \to 126.99$, the residual $0.15$ being a boundary effect); it is only the clipping step that introduces a bias.

\begin{table}[!t]
\centering
\caption{Pixel intensity statistics before and after sharpening.}
\label{tab:hist-stats}
\begin{tabular}{lccc}
\hline
& Original & Sharp (float) & Sharp (clip) \\
\hline
Min   & $0.0$    & $-609.0$ & $0.0$   \\
Max   & $255.0$  & $877.0$  & $255.0$ \\
Mean  & $127.14$ & $126.99$ & $129.68$ \\
Std   & $65.90$  & $136.08$ & $89.58$  \\
\hline
\end{tabular}
\end{table}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/histogram_comparison.png}
  \caption{Histogram comparison: (a)~original image, (b)~sharpened image after clipping to $[0,255]$.}
  \label{fig:histograms}
\end{figure}

\paragraph{Takeaway.}
$K$ enhances contrast because it is algebraically equivalent to the identity minus the discrete Laplacian ($K=\delta-L_4$). At the pixel level, this pushes each value away from its local mean, amplifying edges while leaving flat regions unchanged. The practical limitation is that strong edges can push values outside $[0,255]$, requiring either floating-point storage or clipping with the associated saturation artifacts.

\subsection{Gradient Filters (Sobel)}

In the previous parts, we applied convolution kernels to modify the image (enhancement using the Laplacian). Here we keep the same tool---convolution---but change the goal: instead of sharpening, we want to estimate local intensity variations by computing two derivative-like images, $I_x$ and $I_y$. These two maps can then be combined into a single edge-strength image through the gradient magnitude $\|\nabla I\|$.
\[
I_x = \frac{\partial I}{\partial x}, \qquad I_y = \frac{\partial I}{\partial y}.
\]
These two quantities form the gradient vector
\[
\nabla I = (I_x,\,I_y),
\]
which points toward the direction of maximum increase of intensity. Its magnitude,
\[
\|\nabla I\| = \sqrt{I_x^2 + I_y^2},
\]
is a simple measure of local contrast: it becomes large around edges and sharp transitions.

\subsubsection{From derivatives to discrete filters}

In a digital image we only have samples $I[i,j]$ (pixels), so derivatives must be approximated. A basic idea is finite differences (e.g. $[-1\;\;1]$), but this is very sensitive to noise because it uses only two neighboring pixels.

A common improvement is to combine:
\begin{itemize}
\item a \textit{derivative} in one direction,
\item with a \textit{smoothing} (low-pass) in the orthogonal direction.
\end{itemize}
This is exactly what the Sobel operator does. We use the standard $3\times 3$ kernels:
\begin{equation}
\label{eq:sobel_kernels}
h_x =
\begin{pmatrix}
-1 & 0 & 1 \\
-2 & 0 & 2 \\
-1 & 0 & 1
\end{pmatrix},
\qquad
h_y =
\begin{pmatrix}
-1 & -2 & -1 \\
 0 &  0 &  0 \\
 1 &  2 &  1
\end{pmatrix}.
\end{equation}

They are separable (up to a constant factor), for example
\[
h_x \propto
\begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix}
\begin{pmatrix} -1 & 0 & 1 \end{pmatrix},
\qquad
h_y = h_x^{\top}.
\]
So we can read Sobel as: \textit{smooth with $[1,2,1]$} in the perpendicular direction, then \textit{differentiate with $[-1,0,1]$}. In practice this produces gradient maps that are less noisy than raw finite differences, while still reacting strongly at edges.

\subsubsection{Gradient components and magnitude}

We compute the two components by filtering the image:
\begin{equation}
\label{eq:grad_components_conv}
I_x[i,j] = (I * h_x)[i,j], \qquad I_y[i,j] = (I * h_y)[i,j],
\end{equation}
and then the gradient magnitude:
\begin{equation}
\label{eq:grad_norm}
\|\nabla I[i,j]\| = \sqrt{I_x[i,j]^2 + I_y[i,j]^2}.
\end{equation}

In OpenCV, the filtering function behaves like a correlation (the kernel is not flipped). In this TP context, this does not change the usefulness of the result: it mainly affects the \emph{sign convention} of $I_x$ and $I_y$ depending on the chosen kernel orientation. What matters for edges is that strong transitions create large responses (and the magnitude $\|\nabla I\|$ is independent of the sign).

\subsubsection{Why floating point matters (and how display can go wrong)}

A practical detail is that $I_x$ and $I_y$ are \textbf{signed}: they contain both positive and negative values depending on whether the transition is dark-to-bright or bright-to-dark. Because of that, we should avoid computing the convolution in \texttt{uint8}. Otherwise, negative values are clipped to 0, and half of the information disappears.

In our implementation we therefore compute the convolutions in floating point:
\begin{itemize}
\item either by converting the image to \texttt{float64} before filtering,
\item or by explicitly forcing the output depth in \texttt{cv2.filter2D} (e.g. \texttt{cv2.CV\_64F}).
\end{itemize}
This guarantees that negative responses are preserved and that $\|\nabla I\|$ can exceed the usual $[0,255]$ range.

\subsubsection{Visualization of signed components}

Since $I_x$ and $I_y$ can be negative, their visualization needs a mapping that does not destroy the sign. Typical options are:
\begin{enumerate}
\item \textbf{Incorrect: naive clipping to $[0,255]$}. Negative values become 0, so one edge polarity vanishes.
\item \textbf{Absolute value}. Displays edge strength but removes the sign (direction of transition).
\item \textbf{Shift + rescale}. Map $[\min,\max]$ linearly to $[0,255]$ so that 0 becomes mid-gray.
\item \textbf{Diverging colormap centered at 0 (recommended for interpretation)}. Use a symmetric range
\[
m = \max\left(|\min(I_x)|,\;|\max(I_x)|\right),
\]
\[
\quad \text{and display } I_x \text{ in } [-m, +m],
\]
(and similarly for $I_y$). This makes positive/negative values comparable and avoids a misleading color dominance.
\end{enumerate}

For the magnitude $\|\nabla I\|$, values are non-negative, so a grayscale colormap is enough. However, since the maximum can be much larger than 255, we typically normalize \emph{only for display}. The raw values should be kept if we plan to compare gradients across images or apply consistent thresholds.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/gradient_components.png}
  \caption{Gradient decomposition: (a)~original image, (b)~$I_x$ and (c)~$I_y$ shown with a diverging colormap (red~=~positive, blue~=~negative, white~=~zero), (d)~gradient magnitude $\|\nabla I\|$, (e)~gradient direction $\theta = \arg\nabla I$, (f)~binary edge map ($\|\nabla I\| > \mu+\sigma$).}
  \label{fig:gradient-components}
\end{figure}

Fig.~\ref{fig:gradient-components} illustrates the gradient decomposition on the test image. As introduced in~\cite{manzanera_cours2}, the gradient vector $\nabla I$ encodes two fundamental pieces of local geometry: its \emph{norm} $\|\nabla I\|$ measures the local contrast (the rate of intensity change), while its \emph{argument} $\arg \nabla I$ gives the direction of steepest ascent, orthogonal to the isophote.

These properties are directly visible in the figure. In panel~(b), the horizontal component $I_x$ highlights \textbf{vertical edges}: positive values (red) appear where intensity increases from left to right, and negative values (blue) mark the opposite transition. Panel~(c) shows $I_y$, which correspondingly highlights \textbf{horizontal edges}. For instance, the strong blue and red bands along the flower petals in panel~(b) trace the vertical contours of those structures, while the horizontal top-to-bottom transitions that appear in the fence and the grass are captured in panel~(c).

Panel~(d) combines both components into the scalar magnitude $\|\nabla I\|$: every edge appears bright regardless of its orientation, confirming that the norm acts as a direction-independent measure of local contrast. Panel~(e) displays the gradient direction $\theta = \mathrm{atan2}(I_y,\,I_x)$ using a circular (HSV) colormap; it directly corresponds to the quantity $\arg\nabla I$ from the course~\cite{manzanera_cours2}, and one can verify that edges in the same physical direction share the same hue. Finally, panel~(f) shows a simple binary edge map obtained by keeping only pixels where $\|\nabla I\| > \mu + \sigma$ (a heuristic threshold); this illustrates how the gradient magnitude can be used as a starting point for edge detection---a topic further developed in the course through the zero-crossings of the Laplacian and the Canny detector.
