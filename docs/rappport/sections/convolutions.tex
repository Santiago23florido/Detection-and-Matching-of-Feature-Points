\section{Image Formats and Convolutions}

A discrete two-dimensional convolution is a linear operation that produces an output image by sliding a small matrix, the kernel $K$ of size $(2a+1)\times(2b+1)$, over every pixel of an input image $I$ and computing a weighted sum of the neighborhood, as defined in Eq.~(\ref{eq:conv2d}):

\begin{equation}
\label{eq:conv2d}
(I * K)(y,x)=\sum_{j=-a}^{a}\sum_{i=-b}^{b} I(y+j,\,x+i)\;K(j,i).
\end{equation}

This operation is the computational basis for a range of image-processing tasks such as smoothing, sharpening, edge detection, and feature extraction, all depending on the choice of kernel \cite{gonzalez_woods_dip}.

\subsection{OpenCV and Matplotlib Functions}

The provided code in \texttt{Convolutions.py} relies on several OpenCV and Matplotlib functions whose behavior needs to be understood to interpret the results correctly.

The image is loaded with \texttt{cv2.imread(path, 0)}, where the flag \texttt{0} forces grayscale loading as a single-channel 8-bit array, and the result is cast to \texttt{float64} via \texttt{np.float64()} so that subsequent convolution operations can produce values outside $[0,255]$ without overflow. For copying the image, the code calls \texttt{cv2.copyMakeBorder\allowbreak(img,\allowbreak0,0,0,0,\allowbreak cv2.BORDER\_REPLICATE)} with zero padding on every side, which in practice just creates a deep copy; the \texttt{BORDER\_REPLICATE} flag would replicate edge pixels if positive padding were specified. The optimized convolution is done via \texttt{cv2.filter2D(img, -1, kernel)}, where \texttt{-1} means the output keeps the same depth as the input; internally, this delegates to OpenCV's C++ backend, which can use SIMD and multi-threading \cite{opencv_filter2d}.

An important detail concerns the display conventions: \texttt{cv2.imshow} interprets integer arrays on $\{0,\ldots,255\}$ but floating-point arrays on $[0,1]$, which is why the code divides by $255$ when showing the \texttt{filter2D} result. On the other hand, \texttt{plt.imshow} normalizes the array range to the colormap by default, so to get a consistent grayscale display the code explicitly sets \texttt{vmin=0.0, vmax=255.0} and \texttt{cmap='gray'}.

\subsection{Direct Computation vs OpenCV filter2D}

The code compares two ways of computing the convolution of the grayscale image \texttt{FlowerGarden2.png} ($240\times360$ pixels) with the $3\times3$ sharpening kernel. The first is a direct method that uses a nested Python loop over all interior pixels $(y,x)$ with $1 \le y \le h{-}2$, $1 \le x \le w{-}2$, computing explicitly:

\begin{equation}
\label{eq:direct_val}
v = 5\,I(y,x) - I(y{-}1,x) - I(y,x{-}1) - I(y{+}1,x) - I(y,x{+}1),
\end{equation}

followed by a clamp $I_{\text{out}}(y,x) = \min(\max(v,\,0),\,255)$. The second is a single call to \texttt{cv2.filter2D}, which does the same operation but through OpenCV's compiled C++ backend.

Given the nature of these two implementations, the hypothesis is straightforward: the direct Python loop should be dramatically slower than \texttt{filter2D}, because every pixel iteration goes through the Python interpreter with dynamic type checking, whereas the C++ path processes contiguous memory with compiled instructions and can leverage SIMD vectorization and multi-threading. We would also expect that, for a small image like this one, enabling or disabling multi-threading in OpenCV should not make a large difference, since the overhead of thread synchronization could offset the parallelism gains when the workload per thread is small.

To test this, each method was benchmarked over multiple independent runs (5 for the slow direct method, 20 for the fast \texttt{filter2D} variants), and the median execution time was retained to mitigate warm-up and scheduling noise. A single-thread variant of \texttt{filter2D} was measured separately by calling \texttt{cv2.setNumThreads(1)} before execution.

\begin{table}[!t]
\centering
\caption{Median execution time for the convolution of a $240\times360$ grayscale image with a $3\times3$ kernel.}
\label{tab:conv-timing}
\begin{tabular}{lcr}
\hline
Method & Median time & Runs \\
\hline
Direct (Python loop)       & 0.6987\,s   &  5 \\
\texttt{filter2D} (multi-thread) & 0.329\,ms  & 20 \\
\texttt{filter2D} (1-thread)     & 0.348\,ms  & 20 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/timing_comparison.png}
  \caption{Median execution time comparison (log scale) for the three convolution methods on a $240\times360$ image.}
  \label{fig:timing-comparison}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/timing_boxplot.png}
  \caption{Distribution of execution times across all benchmark runs for each method.}
  \label{fig:timing-boxplot}
\end{figure}

As reported in Table~\ref{tab:conv-timing} and illustrated in Fig.~\ref{fig:timing-comparison}, the direct Python loop takes about $0.70$\,s while \texttt{filter2D} completes in roughly $0.33$\,ms, giving a speedup of approximately $2\,100\times$. This is consistent with the hypothesis: the per-pixel overhead of the Python interpreter is enormous compared to the compiled, vectorized C++ implementation. The difference between the multi-thread and single-thread variants of \texttt{filter2D} is almost negligible ($0.329$\,ms vs $0.348$\,ms), which also confirms our expectation that thread synchronization cost offsets any parallelism benefit at this image size. Moreover, Fig.~\ref{fig:timing-boxplot} shows that the \texttt{filter2D} measurements have very low variance, reflecting the stability of a compiled implementation, while the direct method exhibits more dispersion---likely caused by Python's garbage collector and OS scheduling interrupting the long-running loop.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/time_vs_image_size.png}
  \caption{Execution time as a function of image resolution (25\,\%, 50\,\%, 75\,\%, and 100\,\% of the original $240\times360$ image).}
  \label{fig:time-vs-size}
\end{figure}

To see how these results scale, the benchmark was repeated on resized versions of the image at 25\,\%, 50\,\%, 75\,\%, and 100\,\% of the original resolution. The hypothesis here is that the direct method should grow roughly quadratically with the number of pixels (since it visits every pixel once in a doubly-nested loop), whereas \texttt{filter2D} should remain nearly flat at these small sizes because the actual convolution time is dwarfed by the fixed overhead of the function call. As shown in Fig.~\ref{fig:time-vs-size}, this is exactly what happens: the direct method curve grows steeply as the resolution increases, consistent with $\mathcal{O}(H \cdot W \cdot k^2)$ complexity, while both \texttt{filter2D} variants stay close to the bottom of the plot across all tested resolutions.

\subsection{Visual and Numerical Comparison}

Since both methods are computing the same mathematical convolution, the hypothesis is that their outputs should be identical in the interior of the image, and that any differences should appear only at the borders due to the different ways each method handles boundary pixels. The direct loop simply skips the border (it only iterates from $y=1$ to $h{-}2$) and leaves the original values there via the initial copy, whereas \texttt{filter2D} applies its own default border extrapolation (\texttt{BORDER\_REFLECT\_101}) before computing the convolution, so the border values will differ.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/visual_comparison.png}
  \caption{Visual comparison: original image (top left), direct method result (top right), \texttt{filter2D} result (bottom left), and absolute difference amplified $\times10$ (bottom right).}
  \label{fig:visual-comparison}
\end{figure}

The results in Fig.~\ref{fig:visual-comparison} confirm this. A pixel-wise comparison shows that $98.9\,\%$ of the pixels are numerically identical between the two outputs, with a mean absolute difference of only $0.575$ gray levels. The maximum difference reaches $153$ gray levels, but as visible in the amplified difference heatmap (bottom right of Fig.~\ref{fig:visual-comparison}), these large differences are concentrated exclusively at the image borders. In the interior, both methods produce the same result, which makes sense because the convolution formula is the same---the only divergence comes from boundary handling, not from the computation itself. This confirms that the performance gap discussed in the previous subsection is purely an implementation-level phenomenon and does not affect the mathematical result.

\subsection{Contrast Enhancement by Unsharp Masking}

The convolution kernel provided in the TP code is:

\begin{equation}
\label{eq:kernel}
K=
\begin{pmatrix}
 0 & -1 &  0 \\
-1 &  5 & -1 \\
 0 & -1 &  0
\end{pmatrix}.
\end{equation}

To understand why this kernel enhances contrast, we can decompose it algebraically. Noting that the coefficients sum to $0+(-1)+0+(-1)+5+(-1)+0+(-1)+0 = 1$ and that the central value is $5$ while the four cardinal neighbors are $-1$, this can be rewritten as the identity kernel minus the discrete Laplacian:

\begin{equation}
\label{eq:kernel_decomp}
K = \underbrace{
\begin{pmatrix}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{pmatrix}}_{\delta}
\;-\;
\underbrace{
\begin{pmatrix}
 0 &  1 &  0 \\
 1 & -4 &  1 \\
 0 &  1 &  0
\end{pmatrix}}_{\nabla^{2}}.
\end{equation}

This means the convolution result can be expressed as:

\begin{equation}
\label{eq:unsharp}
I_{\text{out}} = I * K = I - \nabla^{2} I.
\end{equation}

The Laplacian $\nabla^{2} I$ measures the second-order intensity variation: it is positive where a pixel is darker than its neighbors and negative where it is brighter. Subtracting this from the original image amplifies intensity transitions at edges---the dark side of an edge becomes darker and the bright side becomes brighter---while leaving flat regions essentially unchanged (since $\nabla^{2} I \approx 0$ there). This is the classical \emph{unsharp masking} mechanism \cite{gonzalez_woods_dip}.

Based on this decomposition, the hypothesis is as follows: (1)~the sharpened image should show visually crisper edges compared to the original, with the effect concentrated around contours and fine structures; (2)~the global histogram should become wider (higher standard deviation) because edge pixels are pushed to more extreme values; and (3)~the mean intensity should be approximately preserved, since the Laplacian kernel has zero DC response (its coefficients sum to zero) and therefore does not add or remove energy from the image on average.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/kernel_decomposition.png}
  \caption{Visual verification of the kernel decomposition $K = \delta - \nabla^{2}$.}
  \label{fig:kernel-decomp}
\end{figure}

Fig.~\ref{fig:kernel-decomp} shows the decomposition verified numerically: the element-by-element subtraction $\delta - \nabla^{2}$ reconstructs $K$ exactly.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/unsharp_components.png}
  \caption{Unsharp masking components: (a)~original image $f$, (b)~Laplacian $\nabla^{2}f$ (red = positive, blue = negative), (c)~$|\nabla^{2}f|$ rescaled to $[0,255]$, (d)~sharpened result $f - \nabla^{2}f$ after clipping.}
  \label{fig:unsharp-components}
\end{figure}

When applying the decomposition to the actual image, the components are shown in Fig.~\ref{fig:unsharp-components}. The Laplacian map (panel b) reveals the edge structure: transitions between regions of different intensity appear as pairs of positive and negative values, while flat areas stay near zero. As expected from the hypothesis, the sharpened image (panel d) shows noticeably crisper edges, especially visible in the flower contours and the fence structures, while smooth regions remain essentially identical to the original.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/unsharp_1d_profile.png}
  \caption{1D profile at row 120: (a)~original signal, (b)~1D Laplacian, (c)~overlay showing how sharpening amplifies transitions.}
  \label{fig:1d-profile}
\end{figure}

To visualize the mechanism more clearly, Fig.~\ref{fig:1d-profile} shows a horizontal 1D intensity profile extracted at the middle row ($y = 120$), with the 1D Laplacian approximated by $[1,\,-2,\,1]$. At every intensity transition, the Laplacian produces a zero-crossing pattern with positive and negative lobes on either side. Subtracting this from the original causes the signal to ``overshoot'' at each edge: the bright side becomes brighter and the dark side becomes darker, while flat portions of the signal are left untouched. This is precisely the pointwise mechanism behind the contrast enhancement.

\begin{table}[!t]
\centering
\caption{Pixel intensity statistics before and after sharpening.}
\label{tab:hist-stats}
\begin{tabular}{lccc}
\hline
& Original & Sharp (float) & Sharp (clip) \\
\hline
Min   & $0.0$    & $-609.0$ & $0.0$   \\
Max   & $255.0$  & $877.0$  & $255.0$ \\
Mean  & $127.14$ & $126.99$ & $129.68$ \\
Std   & $65.90$  & $136.08$ & $89.58$  \\
\hline
\end{tabular}
\end{table}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/histogram_comparison.png}
  \caption{Histogram comparison: (a)~original (linear), (b)~sharpened clipped (linear) with saturation markers, (c)~sharpened clipped (log scale), (d)~sharpened float before clipping.}
  \label{fig:histograms}
\end{figure}

To evaluate the hypotheses quantitatively, Table~\ref{tab:hist-stats} reports the intensity statistics and Fig.~\ref{fig:histograms} shows the histograms. Looking at the results, the standard deviation increases from $65.90$ to $89.58$ after clipping (a $35.9\,\%$ increase), which confirms that the distribution is indeed wider---the pixel values are more spread out, exactly as predicted. The mean is nearly preserved ($127.14$ vs $129.68$), consistent with the zero-DC-response property of the Laplacian; the small shift of about $2.5$ gray levels is explained by the asymmetric effect of clipping, which removes more negative outliers ($16.36\,\%$ of pixels saturated at $0$) than positive ones ($9.81\,\%$ at $255$), slightly biasing the mean upward.

Before clipping, the sharpened values range from $-609$ to $877$ (Table~\ref{tab:hist-stats}), which shows that the Laplacian subtraction can push pixel values far outside the representable $[0,255]$ interval, especially near strong edges. Panel (d) of Fig.~\ref{fig:histograms} makes this visible: the full float-range histogram extends well beyond the dashed red lines that mark the $[0,255]$ boundaries. When these values are clamped, they accumulate at the boundaries, producing the saturation peaks visible in panel (b). The log-scale view in panel (c) helps see past these peaks and reveals that the body of the distribution follows a shape similar to the original but stretched, which is consistent with the increased standard deviation.

In summary, the kernel $K$ achieves contrast enhancement because its algebraic structure is that of unsharp masking: the identity component preserves the base brightness while the subtracted Laplacian amplifies local intensity differences around edges. The experimental results confirm the three hypotheses: edges are visually sharper, the histogram is wider, and the mean is approximately preserved. The side effect is that pixels near strong edges can exceed the dynamic range, causing saturation at $0$ and $255$ after clipping.

\subsection{Gradient Computation and Display}

Whereas the previous subsection analyzed a second-order differential operator (the Laplacian $\nabla^{2}I$), we now turn to the first-order differential model. In the differential framework~\cite{manzanera_cours1}, the image is treated as a continuous function $f(x,y)$ and its local behavior is studied through its derivatives. At each point, the gradient vector $\nabla I = (I_x,\,I_y)$ points in the direction of maximum intensity change (orthogonal to the isophote), and its magnitude measures the rate of that change. The gradient is the fundamental building block of edge detection: edges correspond to loci where $\|\nabla I\|$ is large, since they separate regions of different intensity.

\subsubsection{Discrete Approximation via Sobel Kernels}

For discrete images the partial derivatives must be approximated by finite differences computed through convolution. The simplest kernels are $[-1\;\;1]$ and $\left[\begin{smallmatrix}-1\\1\end{smallmatrix}\right]$, but these are very sensitive to noise because they act on a single-pixel baseline. As presented in~\cite{manzanera_cours1}, a standard improvement is to combine the derivative in one direction with a smoothing filter in the orthogonal direction. The Sobel operator implements this idea with the following $3\times3$ masks:

\begin{equation}
\label{eq:sobel_x}
h_x =
\begin{pmatrix}
-1 & 0 & 1 \\
-2 & 0 & 2 \\
-1 & 0 & 1
\end{pmatrix},
\qquad
h_y =
\begin{pmatrix}
-1 & -2 & -1 \\
 0 &  0 &  0 \\
 1 &  2 &  1
\end{pmatrix}.
\end{equation}

Each kernel is separable: $h_x$ can be factored as a column smoothing vector $[1,\,2,\,1]^{T}$ times a row derivative $[-1,\,0,\,1]$, and $h_y$ is its transpose. The smoothing component ($[1,\,2,\,1]$ is a binomial approximation to a Gaussian) attenuates noise in the direction perpendicular to the derivative~\cite{gonzalez_woods_dip}, producing edge maps that are thicker but better centered (zero-phase response) and less noisy than the raw $[-1\;\;1]$ difference.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.75\linewidth]{imgs/convolutions/gradient_kernels.png}
  \caption{Sobel kernels $h_x$ (horizontal derivative) and $h_y$ (vertical derivative) with annotated coefficients.}
  \label{fig:gradient-kernels}
\end{figure}

Fig.~\ref{fig:gradient-kernels} shows the two kernels with their annotated coefficient values. Note that both have zero-sum coefficients, which means their Fourier transforms vanish at the origin: a derivative kernel has no DC response, so a constant image produces zero output---consistent with the fact that a constant function has zero derivative everywhere.

The gradient components and the Euclidean norm are then computed as:
\begin{equation}
\label{eq:gradient_components}
I_x = I * h_x, \qquad I_y = I * h_y,
\end{equation}
\begin{equation}
\label{eq:gradient_norm}
\|\nabla I\| = \sqrt{I_x^{2} + I_y^{2}}.
\end{equation}

In the implementation, both convolutions are performed with \texttt{cv2.filter2D} using \texttt{cv2.CV\_64F} as the output depth. This is the first and most critical precaution: if the default depth \texttt{-1} were used on a \texttt{uint8} or \texttt{float64}-clipped image, all negative gradient values would be silently clamped to zero, destroying half of the edge information. Since the Sobel kernels are antisymmetric, the output range is symmetric around zero; for the test image we measure $I_x \in [-879,\,730]$ and $I_y \in [-1012,\,974]$.

\subsubsection{Gradient Components}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/gradient_components.png}
  \caption{Gradient analysis: (a)~original image, (b)~$I_x$ and (c)~$I_y$ with divergent colormap (red = positive, blue = negative), (d)~gradient norm $\|\nabla I\|$, (e)~gradient direction $\theta = \mathrm{atan2}(I_y,I_x)$, (f)~thresholded edges ($\|\nabla I\| > \mu + \sigma$).}
  \label{fig:gradient-components}
\end{figure}

Fig.~\ref{fig:gradient-components} presents a six-panel overview of the gradient decomposition applied to the test image. In panel~(b), the horizontal derivative $I_x$ highlights vertical edges: positive values (red) appear on transitions from dark to bright as the $x$ coordinate increases, while negative values (blue) mark the opposite transition. Panel~(c) shows $I_y$, which correspondingly highlights horizontal edges. Since the Sobel operator includes orthogonal smoothing, these edge maps are cleaner and better localized than what raw finite differences would produce.

The Euclidean norm $\|\nabla I\|$ (panel~d) combines both components into a single scalar measure of edge strength. It is always non-negative, so it can be displayed directly with a standard grayscale colormap. The gradient direction $\theta = \mathrm{atan2}(I_y,\,I_x)$ (panel~e) is shown with a circular (HSV) colormap, where the hue encodes the angle from $-\pi$ to $\pi$; this reveals the orientation of each edge in the image. Finally, panel~(f) shows a binary edge map obtained by thresholding the norm at $\mu + \sigma$ (where $\mu$ and $\sigma$ are the mean and standard deviation of $\|\nabla I\|$), giving a simple but effective visualization of the most salient contours.

\begin{table}[!t]
\centering
\caption{Statistics of gradient components on the $240\times360$ test image.}
\label{tab:gradient-stats}
\begin{tabular}{lrrrr}
\hline
Component & Min & Max & Mean & Std \\
\hline
$I_x\;(\partial I/\partial x)$ & $-879$ & $730$  & $-3.66$  & $122.60$ \\
$I_y\;(\partial I/\partial y)$ & $-1012$ & $974$ & $-1.02$  & $183.08$ \\
$\|\nabla I\|$                 & $0$     & $1070$ & $146.64$ & $164.50$ \\
\hline
\end{tabular}
\end{table}

Table~\ref{tab:gradient-stats} reports the numerical statistics. The means of $I_x$ and $I_y$ are close to zero, which is expected since the Sobel kernels have zero DC response and positive and negative transitions tend to balance over the entire image. The standard deviations ($122.60$ for $I_x$, $183.08$ for $I_y$) indicate that the image has somewhat stronger vertical intensity transitions than horizontal ones. The gradient norm reaches a maximum of $1070$, well above the $[0,255]$ range---this underlines the need for floating-point computation and careful normalization for display.

\subsubsection{Display Precautions}

Since $I_x$ and $I_y$ are signed quantities, displaying them correctly requires specific precautions. Fig.~\ref{fig:display-precautions} compares four strategies side by side for both components:

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/gradient_display_precautions.png}
  \caption{Four display strategies for the signed gradient components $I_x$ (top row) and $I_y$ (bottom row): (a)~naive clipping to $[0,255]$ (incorrect), (b)~absolute value, (c)~shift and rescale, (d)~divergent colormap (recommended).}
  \label{fig:display-precautions}
\end{figure}

\begin{enumerate}
\item \textbf{Naive clipping to $[0,255]$} (column~a): all negative values are set to zero, so transitions in one direction are completely invisible. This is incorrect and would lead to the false conclusion that edges exist only in one direction of the intensity change. In the figure this manifests as images where roughly half the edges appear to be missing.

\item \textbf{Absolute value $|I_x|$, $|I_y|$} (column~b): edge strength is preserved, but the sign---which encodes the direction of the intensity transition (dark-to-bright vs.\ bright-to-dark)---is lost. This is acceptable when only edge magnitude matters, but it discards useful orientation information.

\item \textbf{Shift and rescale} (column~c): the full range $[\min,\,\max]$ is linearly mapped to $[0,255]$, so that zero maps to a mid-gray value. All information is preserved, and the result can be saved as a standard 8-bit image. The disadvantage is that the visual mapping between gray levels and physical quantities is not immediately intuitive.

\item \textbf{Divergent colormap centered at zero} (column~d, recommended): a symmetric colormap such as \texttt{RdBu\_r} is used with \texttt{TwoSlopeNorm(vcenter=0)}, so that zero maps to white, positive values to red, and negative values to blue. This simultaneously preserves sign information, edge strength, and provides an intuitive reading of the gradient direction.
\end{enumerate}

The gradient norm $\|\nabla I\|$ is always non-negative, so a standard grayscale display with automatic or manual normalization suffices. No special precaution is required for the norm, only for the signed components.

In summary, the two essential precautions for a correct display of gradient components are: (1)~using a floating-point output depth (\texttt{cv2.CV\_64F}) in the convolution to preserve negative values, and (2)~choosing a visualization strategy that does not discard or misrepresent the sign. The divergent colormap approach satisfies both requirements and is therefore the recommended method.

\subsubsection{1D Profile Analysis}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{imgs/convolutions/gradient_1d_profile.png}
  \caption{1D gradient profile at row 120: (a)~original intensity, (b)~horizontal gradient $I_x$ with positive/negative regions shaded, (c)~gradient norm $\|\nabla I\|$.}
  \label{fig:gradient-1d-profile}
\end{figure}

To complement the 2D visualizations, Fig.~\ref{fig:gradient-1d-profile} shows a horizontal 1D profile extracted at the middle row ($y=120$). Panel~(a) displays the original intensity profile, where abrupt transitions between regions of different brightness are clearly visible. Panel~(b) shows the horizontal derivative $I_x$: each intensity transition produces a signed peak whose polarity indicates the direction of the change---positive (red shading) for dark-to-bright and negative (blue shading) for bright-to-dark. In flat regions, $I_x$ stays close to zero. Panel~(c) shows the gradient norm, which converts these signed peaks into positive peaks of edge strength, with the strongest responses at the locations of the sharpest transitions.

This 1D view makes explicit the relationship between the first-order differential model and edge detection: the gradient operator acts as a high-pass filter that responds to intensity transitions, and the signed components preserve the directionality of each transition while the norm provides a direction-independent measure of edge strength.
